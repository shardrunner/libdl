{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "    \n",
    "# MLIB - Deep Learning library\n",
    "\n",
    "## Structure\n",
    "\n",
    "* modular OO network with a central class as interface\n",
    "\n",
    "* no maxpool, but instead filter with stride\n",
    "\n",
    "* mostly Matrix Ops for everything but the convolution layer, there im2col\n",
    "\n",
    "* images stored in columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "* dataset from an Intel ML challenge, now on kaggle\n",
    "\n",
    "* contains 17000 labeled images from streets, mountains, seas, buildings, forests, glaciers\n",
    "\n",
    "* bigger MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "* started with far to big network, which led to crashes\n",
    "\n",
    "* began from new and took a really small network and sample size\n",
    "\n",
    "* scaled up from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Hyperparameter\n",
    "\n",
    "* Batch Size: 128\n",
    "\n",
    "* Layers: 6\n",
    "\n",
    "    * Convolution Layer: 4\n",
    "    \n",
    "    * FullyConnected Layer: 2\n",
    "    \n",
    "* MultiClassCrossEntropy Loss\n",
    "\n",
    "* Adam with standard values\n",
    "\n",
    "* 873840 learnable parameter\n",
    "\n",
    "* Similar to LeNet, VGGNet: Filter -> Pooling -> Filter -> Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "* start small to test, then expand\n",
    "\n",
    "* pixel dimension > 50 is not worth it (for my system/implementation)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MlibWrapper as mlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "from math import floor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change input pixel dimensions here\n",
    "img_dim=50\n",
    "\n",
    "# Max number of images\n",
    "img_train_num=14034\n",
    "img_test_num=3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import images\n",
    "def get_images(directory,number):\n",
    "    images=np.empty([img_dim*img_dim*3,number],dtype=np.single,order='F')    \n",
    "    labels=np.empty([number],dtype=np.intc,order='F')\n",
    "    label=0\n",
    "    position=0\n",
    "\n",
    "    path=Path(directory)\n",
    "    \n",
    "    for folder in path.iterdir():\n",
    "        print(folder)\n",
    "        if folder.name == 'buildings':\n",
    "            label = 0\n",
    "        elif folder.name == 'forest':\n",
    "            label = 1\n",
    "        elif folder.name == 'glacier':\n",
    "            label = 2\n",
    "        elif folder.name == 'mountain':\n",
    "            label = 3\n",
    "        elif folder.name == 'sea':\n",
    "            label = 4\n",
    "        elif folder.name == 'street':\n",
    "            label = 5\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "\n",
    "        img_count=0\n",
    "        for image_file in folder.iterdir():\n",
    "            # distribute images evenly -> block if one class reaches 1/6 of all images\n",
    "            if (img_count == number/6):\n",
    "                break\n",
    "            img=Image.open(str(image_file))\n",
    "            img=img.resize((img_dim,img_dim),Image.BILINEAR)\n",
    "            # normalize image data\n",
    "            img=np.asarray((img))/255.0\n",
    "            images[:,position]=np.reshape(img,(img_dim*img_dim*3),order='F')\n",
    "            \n",
    "            labels[position]=label\n",
    "            position=position+1\n",
    "            img_count=img_count+1\n",
    "\n",
    "    print(\"Finished importing\")\n",
    "    # permutation of data\n",
    "    idx = np.random.permutation(len(labels))\n",
    "    return (images[:,idx], labels[idx])\n",
    "\n",
    "# get class name for label\n",
    "def get_class_label(class_code):\n",
    "    labels = {0:'buildings', 1:'forest', 2:'glacier', 3:'mountain', 4:'sea', 5:'street'}\n",
    "    return labels[class_code]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of images to import and train/test on (has to be divisible by 6)\n",
    "img_train_num=9900\n",
    "img_test_num=1200\n",
    "# Set the correct data location for the inputs\n",
    "train_images, train_labels = get_images('../data/seg_train/',img_train_num)\n",
    "test_images, test_labels = get_images('../data/seg_test',img_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of testing Images:\",train_images.shape)\n",
    "print(\"Shape of testing Labels:\",train_labels.shape)\n",
    "print(\"Shape of training Images:\",test_images.shape)\n",
    "print(\"Shape of training Labels:\",test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example picture\n",
    "pic=9\n",
    "print(get_class_label(train_labels[pic]))\n",
    "plt.imshow(np.reshape(train_images[:,pic],(img_dim,img_dim,3),order='F'))\n",
    "print(train_images[:,pic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(2,2)\n",
    "f.subplots_adjust(0,0,3,3)\n",
    "for i in range(0,2,1):\n",
    "    for j in range(0,2,1):\n",
    "        rnd_number = randint(0,len(train_images[0,:]))\n",
    "        ax[i,j].imshow(np.reshape(train_images[:,rnd_number],(img_dim,img_dim,3),order='F'))\n",
    "        ax[i,j].set_title(get_class_label(train_labels[rnd_number]))\n",
    "        ax[i,j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The used network architecture\n",
    "# 6 Layers, 4 Conv (Relu+UniformHe), 2 FC (Tanh+Softmax,UniformXavier), MultiClassCrossEntropy Loss, Adam, 873840 parameter\n",
    "# Similar to LeNet, VGGNet: Filter -> Pooling -> Filter -> Pooling\n",
    "NN = mlib.NeuralNetwork()\n",
    "NN.use_multiclass_loss()\n",
    "NN.add_conv_layer(50,50,3,3,3,12,1,0)\n",
    "NN.add_conv_layer(48,48,12,2,2,18,2,0)\n",
    "NN.add_conv_layer(24,24,18,3,3,36,1,0)\n",
    "NN.add_conv_layer(22,22,36,2,2,54,2,0)\n",
    "NN.add_fc_layer(11*11*54,128)\n",
    "NN.add_output_layer(128,6)\n",
    "NN.layer_size()\n",
    "NN.check_network(img_dim*img_dim*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set visualization parameters 0\n",
    "accuracy=[]\n",
    "error=[]\n",
    "val_accuracy=[]\n",
    "val_error=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network, only 1/3 of the validation set is used each time for validation\n",
    "epochs=10\n",
    "batch_size=128\n",
    "iterations=floor(img_train_num/batch_size)\n",
    "print(\"Iterations per epoch: {}\".format(iterations))\n",
    "size = int(img_test_num/3)\n",
    "\n",
    "for j in range(0,epochs):\n",
    "    print(\"Epoch: {}\".format(j))\n",
    "    for i in range(0,iterations):\n",
    "        #print(\"{}\".format(i),end = '')\n",
    "        NN.train_batch(train_images[:,i*batch_size:(i+1)*batch_size],train_labels[i*batch_size:(i+1)*batch_size])\n",
    "        if(j % 1 ==0 and i ==0):\n",
    "            #print(\"Iteration: {}\".format(i))\n",
    "            acc=NN.get_current_accuracy(train_labels[i*batch_size:(i+1)*batch_size])\n",
    "            print(\"Accuracy: {}\".format(acc))\n",
    "            err=NN.get_current_error(train_labels[i*batch_size:(i+1)*batch_size])\n",
    "            print(\"Error: {}\".format(err))\n",
    "            accuracy.append(acc)\n",
    "            error.append(err)\n",
    "            \n",
    "            k = epochs % 3            \n",
    "            NN.feed_forward_py(test_images[:,k*size:(k+1)*size])\n",
    "            acc=NN.get_current_accuracy(test_labels[k*size:(k+1)*size])\n",
    "            print(\"Val Accuracy: {}\".format(acc))\n",
    "            err=NN.get_current_error(test_labels[k*size:(k+1)*size])\n",
    "            print(\"Val Error: {}\".format(err))\n",
    "            val_accuracy.append(acc)\n",
    "            val_error.append(err)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network weights and graph/visualization data from previously pickelt data (see cell below)\n",
    "with open('../weights.pickle', 'rb') as handle:\n",
    "    weights = pickle.load(handle)\n",
    "with open('../bias.pickle', 'rb') as handle:\n",
    "    bias = pickle.load(handle)\n",
    "with open('../graphs.pickle', 'rb') as handle:\n",
    "    accuracy,error,val_accuracy,val_error =pickle.load(handle)\n",
    "with open('../info.pickle','rb') as handle:\n",
    "    info = pickle.load(handle)\n",
    "print(info)\n",
    "    \n",
    "for i in range(0,NN.layer_size()):\n",
    "    NN.set_layer_weights(weights[i],i)\n",
    "    NN.set_layer_bias(bias[i],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(accuracy)\n",
    "plt.plot(val_accuracy)\n",
    "plt.title('network accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(error)\n",
    "plt.plot(val_error)\n",
    "plt.title('network loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.feed_forward_py(test_images[:,0:300])\n",
    "current_pred=NN.get_current_prediction()\n",
    "prediction=NN.get_predicted_classes(current_pred)\n",
    "f,ax = plt.subplots(2,2)\n",
    "f.subplots_adjust(0,0,3,3)\n",
    "for i in range(0,2,1):\n",
    "    for j in range(0,2,1):\n",
    "        rnd_index = randint(0,len(prediction))\n",
    "        ax[i,j].imshow(np.reshape(test_images[:,rnd_index],(img_dim,img_dim,3),order='F'))\n",
    "        ax[i,j].set_title(\"pred: {}, real: {}\".format(get_class_label(prediction[rnd_index]),get_class_label(test_labels[rnd_index])))\n",
    "        ax[i,j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store network weights and graph/visualization data\n",
    "weights=[]\n",
    "bias=[]\n",
    "for i in range(0,NN.layer_size()):\n",
    "    weights.append(NN.get_layer_weights(i))\n",
    "    bias.append(NN.get_layer_bias(i))\n",
    "    \n",
    "with open('../weights.pickle', 'wb') as handle:\n",
    "    pickle.dump(weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../bias.pickle', 'wb') as handle:\n",
    "    pickle.dump(bias, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../graphs.pickle', 'wb') as handle:\n",
    "    pickle.dump([accuracy,error,val_accuracy,val_error], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../info.pickle', 'wb') as handle:\n",
    "    pickle.dump({'img_dim':img_dim,'img_train_num':img_train_num,'img_test_num':img_test_num,'batch_size':batch_size,'epochs':len(accuracy)}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
